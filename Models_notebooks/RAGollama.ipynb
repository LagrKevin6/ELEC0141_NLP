{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "excel_path = 'Policy.xlsx'\n",
    "# Loading policy from Excel\n",
    "policy_df = pd.read_excel(excel_path,sheet_name= 0, engine='openpyxl')\n",
    "prompt_df = pd.read_excel(excel_path,sheet_name= 1, engine='openpyxl')\n",
    "label_df = pd.read_excel(excel_path,sheet_name= 2, engine='openpyxl')\n",
    "\n",
    "policy_scripts = policy_df['Policy Scripts'].tolist()\n",
    "def parse_labels(label_str):\n",
    "    try:\n",
    "        # Safely evaluate the string as a Python literal (list in this case)\n",
    "        return ast.literal_eval(label_str)\n",
    "    except ValueError:\n",
    "        # In case of an error (e.g., malformed string), return an empty list or handle accordingly\n",
    "        return []\n",
    "\n",
    "# Apply the conversion to each row in the 'CorrectLabels' column\n",
    "label_df['True'] = label_df['label list'].apply(parse_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "# Gemma 2B\n",
    "Token = \"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\"\n",
    "global tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", token=Token)\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\",token=Token,\n",
    "#    trust_remote_code=True, device_map=\"cpu\", torch_dtype=torch.float16)\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "\n",
    "#tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "#model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ollama in c:\\users\\zhujy\\appdata\\roaming\\python\\python311\\site-packages (0.1.9)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\zhujy\\appdata\\roaming\\python\\python311\\site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in d:\\anaconda\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in d:\\anaconda\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\zhujy\\appdata\\roaming\\python\\python311\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in d:\\anaconda\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.4)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\zhujy\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "embedding = []\n",
    "for i in range(len(policy_df)):\n",
    "    policy_emb = ollama.embeddings(model='nomic-embed-text', prompt=policy_scripts[i])\n",
    "    embedding.append(policy_emb['embedding'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "# Build FAISS index\n",
    "doc_embeddings = np.array(embedding, dtype='float32')\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Sample query\n",
    "query = prompt_df['Prompt'][5]\n",
    "\n",
    "# Generate BERT embedding for the query\n",
    "query_embedding = ollama.embeddings(model='nomic-embed-text', prompt=query)['embedding']\n",
    "query_array = np.array(query_embedding)\n",
    "query_embedding = query_array.reshape(1, 768)\n",
    "\n",
    "k = 10\n",
    "\n",
    "# Perform similarity search\n",
    "_, indices = index.search(query_embedding, k)\n",
    "similar_documents_indices = indices.flatten().tolist()\n",
    "\n",
    "# Get similar documents\n",
    "similar_documents = [policy_scripts[i] for i in similar_documents_indices]\n",
    "\n",
    "# Print the actual documents\n",
    "print(query)\n",
    "for idx in indices[0]:\n",
    "    print(f\"\\n### Retrieved Document {idx}:\\n{policy_scripts[idx][:166]}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_retrieval(search_result, true_labels):\n",
    "    \"\"\"\n",
    "    Check if all true labels are contained within the predicted labels.\n",
    "    Args:\n",
    "    predicted_labels (list): The labels retrieved by the search model.\n",
    "    true_labels (list): The correct labels listed in the Excel file.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if all true labels are in the predicted labels, False otherwise.\n",
    "    \"\"\"\n",
    "    #force the input to be sets;\n",
    "    y_pred = set(search_result)\n",
    "    y_true = set(true_labels)\n",
    "\n",
    "    # check coverage\n",
    "    correct = y_true.intersection(y_pred)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    coverage = len(correct) / len(y_true)\n",
    "\n",
    "    return y_true.issubset(y_pred), coverage\n",
    "\n",
    "def k_top_search(upper_thres, tokenizer, index, prompt_df, label_df, lower_thres = 5):\n",
    "    \"\"\"\n",
    "    loop through all k in a range, from lower thres (5 by default) to upper thres,\n",
    "    giving a figure showing the accuracy, coverage and average token number over different k\n",
    "    Args:\n",
    "    upper_thres (int): Number of top searches upper limit\n",
    "    lower_thres (int): Number of top searches lower limit\n",
    "    vectorizer: Tfidvectorizer, fitted\n",
    "    index: faiss object after index addition\n",
    "    prompt_df: pd df, must contain 'Prompt' column for queries\n",
    "    label_df: pd df, must contain 'True' column for true labels\n",
    "    Returns:\n",
    "    null\n",
    "    \"\"\"\n",
    "    # lists for plots\n",
    "    accuracies = []\n",
    "    coverages = []\n",
    "    num_tokens = []\n",
    "\n",
    "    for k in range(lower_thres, upper_thres+1):\n",
    "        indices_list = []\n",
    "        token_count = 0\n",
    "\n",
    "        for query in prompt_df['Prompt']:\n",
    "\n",
    "            query_embedding = ollama.embeddings(model='nomic-embed-text', prompt=query)['embedding']\n",
    "            query_array = np.array(query_embedding)\n",
    "            query_embedding = query_array.reshape(1, 768)\n",
    "            _, indices = index.search(query_embedding, k)\n",
    "            indices_list.append(indices.flatten().tolist())\n",
    "\n",
    "            # combining the full query with full searched docs\n",
    "            combined_query = query\n",
    "            for temp in indices.flatten().tolist():\n",
    "                combined_query = combined_query + ' ' + policy_scripts[temp]\n",
    "\n",
    "            # tokenize\n",
    "            tokens = tokenizer.tokenize(combined_query)\n",
    "            \n",
    "            # Return the number of tokens\n",
    "            token_count += len(tokens)\n",
    "\n",
    "        # Adding search results for further check\n",
    "        prompt_df['TopIndices'] = indices_list\n",
    "\n",
    "\n",
    "        #evaluate\n",
    "        accu_count = 0\n",
    "        accu_cover_count = 0\n",
    "        for i in range(len(prompt_df)):\n",
    "            temp, cover = evaluate_retrieval(prompt_df['TopIndices'][i] , label_df['True'][i])\n",
    "            accu_count += int(temp)\n",
    "            accu_cover_count += cover\n",
    "\n",
    "        accuracy_1 = accu_count/len(prompt_df)\n",
    "        coverage_1 = accu_cover_count/len(prompt_df)\n",
    "        print(f\"For top {k} searches:\\nAccuracy of search results containing all correct labels: {accuracy_1 * 100},\\n Average coverage of correct labels: {coverage_1 * 100}\")\n",
    "\n",
    "        token_1 = token_count/len(prompt_df)\n",
    "        print(f\"Average tokens combining the query and retrieved docs: {token_1}\")\n",
    "\n",
    "        # Add to the lists\n",
    "        accuracies.append(accuracy_1)\n",
    "        coverages.append(coverage_1)\n",
    "        num_tokens.append(token_1)\n",
    "        \n",
    "    return accuracies, coverages, num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "upper_search = 20\n",
    "a,b,c = k_top_search(upper_thres= upper_search,tokenizer=tokenizer, index = index, prompt_df = prompt_df, label_df = label_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot acc and cover.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(5, upper_search+1), a, label='Accuracy (Fully Match)')\n",
    "plt.plot(range(5, upper_search+1), b, label='Average Coverage')\n",
    "plt.xlabel('Num of Searched Results')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Fully Match Accuracy and Average Coverage vs. Num of Searched Results Included')\n",
    "plt.legend()\n",
    "plt.savefig('images/acc_2.png')  # Save the plot to a specific path\n",
    "#plt.close()  # Close the figure to release memory\n",
    "\n",
    "\n",
    "# Plot number of tokens\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(5, upper_search+1), c, color='green')\n",
    "plt.xlabel('Num of Searched Results')\n",
    "plt.ylabel('Total Number of Tokens')\n",
    "plt.title('Number of Tokens vs. k')\n",
    "plt.savefig('images/num_tokens_2.png')  # Save the plot to a specific path\n",
    "#plt.close()  # Close the figure to release memory\n",
    "indices_list = []\n",
    "token_count = 0\n",
    "for query in prompt_df['Prompt']:\n",
    "\n",
    "    query_vector = vectorizer.transform([query]).toarray()\n",
    "    query_vector = np.array(query_vector, dtype='float32')\n",
    "    _, indices = index.search(query_vector, k)\n",
    "    indices_list.append(indices.flatten().tolist())\n",
    "\n",
    "    combined_query = query\n",
    "    for temp in indices.flatten().tolist():\n",
    "        combined_query = combined_query + ' ' + policy_scripts[temp]\n",
    "\n",
    "    # tokenize\n",
    "    tokens = tokenizer.tokenize(combined_query)\n",
    "    \n",
    "    # Return the number of tokens\n",
    "    token_count += len(tokens)\n",
    "\n",
    "# Adding search results for further check\n",
    "prompt_df['Top3Indices'] = indices_list\n",
    "\n",
    "# Save to new sheet, only run once\n",
    "with pd.ExcelWriter(excel_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "    prompt_df[['Prompt','Top3Indices']].to_excel(writer, sheet_name='Eval_result1')\n",
    "\n",
    "accu_count = 0\n",
    "accu_cover_count = 0\n",
    "for i in range(len(prompt_df)):\n",
    "    temp, cover = evaluate_retrieval(prompt_df['Top3Indices'][i] , label_df['True'][i])\n",
    "    accu_count += int(temp)\n",
    "    accu_cover_count += cover\n",
    "\n",
    "accuracy_1 = accu_count/len(prompt_df)\n",
    "coverage_1 = accu_cover_count/len(prompt_df)\n",
    "print(f\"Accuracy of search results containing all correct labels: {accuracy_1 * 100},\\n Average coverage of correct labels: {coverage_1 * 100}\")\n",
    "\n",
    "token_1 = token_count/len(prompt_df)\n",
    "print(f\"Average tokens combining the query and retrieved docs: {token_1}\")\n",
    "# make a Query\n",
    "query_text = \"Does GNEI provide travel insurance? Receipts and prior approval required\"\n",
    "query_vector = vectorizer.transform([query_text]).toarray()\n",
    "query_vector = np.array(query_vector, dtype='float32')\n",
    "\n",
    "# Searching the index\n",
    "k = 10  # Number of nearest neighbors\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "print(\"Distances:\", distances.flatten())\n",
    "print(\"Indices:\", indices.flatten())\n",
    "\n",
    "indices[0]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve and plot the distances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(np.sort(indices[0]), distances[0])\n",
    "plt.title('FAISS Retrieval Distances')\n",
    "plt.xlabel('Index of Retrieved Documents')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Print the actual documents\n",
    "for idx in indices[0]:\n",
    "    print(f\"\\n### Retrieved Document {idx}:\\n{policy_scripts[idx][:100]}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
